# A unique identifier for the head node and workers of this cluster.
cluster_name: slurm-cluster

# The maximum number of workers nodes to launch in addition to the head
# node.
min_workers: 0
max_workers: 2

# The autoscaler will scale up the cluster faster with higher upscaling speed.
# E.g., if the task requires adding more nodes then autoscaler will gradually
# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
# This number should be > 0.
upscaling_speed: 1.0

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

provider:
  type: external
  module: ray.autoscaler._private.slurm.node_provider.NodeProvider
  temp_folder_name: [_DEPLOY_RAY_PATH_]/autoscaler/slurm/temp_script/ # Need to be absolute path
  template_path: [_DEPLOY_RAY_TEMPLATE_PATH_] # Need to be absolute path
  head_ip: "127.0.0.1" # TODO:
  gcs_port: "6379"
  ray_client_port: "10001"
  dashboad_port : "8265"

# Specify the type for the ray head node (as configured below).
head_node_type: head_node

# Specify the allowed pod types for this ray cluster and the resources they provide.
available_node_types:
  head_node:
    max_workers: 0 # do not modify
    resources: {"CPU": [_DEPLOY_HEAD_CPUS_], "GPU": [_DEPLOY_HEAD_GPUS_]} # will be used by autoscaler scheduler
    node_config:
      head_node: 1 # needed by my create node. Do not modify 
      under_slurm: 1 # whether the head node should be started under slurm
      head_node_name : "head" # TODO: only useful when launching head under slurm
      init_commands:
        - conda activate env_name # TODO:
      additional_slurm_commands: 
        - "#SBATCH --reservation=username"
        - "#SBATCH --partition=cpu"
  
  worker_node:
    # Minimum number of Ray workers of this type.
    min_workers: 0
    # Maximum number of Ray workers of this type. Takes precedence over min_workers.
    max_workers: 2

    resources: {"CPU": [_DEPLOY_WORKER_CPUS_], "GPU": [_DEPLOY_WORKER_GPUS_]} # will be used by autoscaler scheduler
    node_config: 
      head_node: 0 # needed by my create node. Do not modify 
      under_slurm: 1 # doesn't matter. Worker node has to under slurm
      init_commands:
        - conda activate env_name # TODO:
      additional_slurm_commands: 
        - "#SBATCH --reservation=username"
        - "#SBATCH --partition=cpu"
  

# Do not modify the fields below!

# Empty setup commands for cluster launcher's checking
initialization_commands: []
setup_commands: []
head_setup_commands: []
worker_setup_commands: []
file_mounts: {}
cluster_synced_files: []

# Should be empty for Slurm provider. Fill the commands in node types instead
head_start_ray_commands: []

# Should be empty for Slurm provider. Fill the commands in node types instead
worker_start_ray_commands: []
